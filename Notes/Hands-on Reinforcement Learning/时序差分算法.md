## 时序差分（Temporal Difference, TD）

对于大部分现实场景，是写不出状态转移函数的，也就是只能通过与环境的交互来采集数据——无模型强化学习

输入：智能体的动作，输出：奖励和下一个状态给智能体

不同于动态规划，TD没有状态转移方程（但我觉得依然有奖励函数，内化在环境里面）

Sarsa算法和Q-learning都是基于TD，不同的是，Sarsa采用在线策略（像水龙头洗手），从而更保守、期望更高；Q-learning采用离线策略（像水盆里洗手），从而更探索、期望更低

我认为并不能一概而论地说哪种算法更好，只能说在不同场景下，不同方法的侧重点不同

### Sarsa算法

使用表格Q存储当前策略下所有<状态，动作>对的价值。

#### 采用时序差分
只利用**下一步**的奖励和**下一步**状态的价值估计（有偏）

>比较远离悬崖

#### 采用多步时序差分
只利用**之后n步**的奖励和**之后n步**状态的价值估计（借鉴蒙特卡洛的思想，但又不是吸收之后每一步，一种trade-off）
>在离悬崖最远的路上走

### Q-learning
离线策略算法能够重复使用过往训练样本，往往具有更小的样本复杂度。

在代码中和Sarsa唯一的区别在于update采用下一动作状态中价值最大的，而不是仅仅是采用下一状态和下一动作。

### Dyna-Q
基于Q-learning，不同的是，不是每次Q-learning都是与环境交互，而是Q-learning一次，将(s, a, r, s')存入字典，然后反刍 n_planning （超参）次，所谓反刍，就是从字典中随机选取之前的(s, a, r, s')进行Q-learning。

结果显示反刍次数越多，回报收敛越快。但这取决于环境是否是确定性的，以及环境模型的精度。
在上述悬崖漫步环境中，状态的转移是完全确定性的，构建的环境模型的精度是最高的，所以可以通过增加 Q-planning 步数来直接降低算法的样本复杂度。
