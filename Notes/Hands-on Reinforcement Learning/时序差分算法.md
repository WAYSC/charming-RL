## 时序差分（Temporal Difference, TD）

对于大部分现实场景，是写不出状态转移函数的，也就是只能通过与环境的交互来采集数据——无模型强化学习

输入：智能体的动作，输出：奖励和下一个状态给智能体

不同于动态规划，TD没有状态转移方程（但我觉得依然有奖励函数，内化在环境里面）

Sarsa算法和Q-learning都是基于TD，不同的是，Sarsa采用在线策略（像水龙头洗手），从而更保守、期望更高；Q-learning采用离线策略（像水盆里洗手），从而更探索、期望更低

我认为并不能一概而论地说哪种算法更好，只能说在不同场景下，不同方法的侧重点不同

### Sarsa算法

使用表格Q存储当前策略下所有<状态，动作>对的价值。

#### 采用时序差分
只利用**下一步**的奖励和**下一步**状态的价值估计（有偏）

>比较远离悬崖

#### 采用多步时序差分
只利用**之后n步**的奖励和**之后n步**状态的价值估计（借鉴蒙特卡洛的思想，但又不是吸收之后每一步，一种trade-off）
>在里悬崖最远的路上走

### Q-learning
离线策略算法能够重复使用过往训练样本，往往具有更小的样本复杂度。

在代码中和Sarsa唯一的区别在于update采用下一动作状态中价值最大的，而不是仅仅是采用下一状态和下一动作。
