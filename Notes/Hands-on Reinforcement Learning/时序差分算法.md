## 时序差分（Temporal Difference, TD）

对于大部分现实场景，是写不出状态转移函数的，也就是只能通过与环境的交互来采集数据——无模型强化学习

输入：智能体的动作，输出：奖励和下一个状态给智能体

不同于动态规划，TD没有状态转移方程（但我觉得依然有奖励函数，内化在环境里面）

Sarsa算法和Q-learning都是基于TD，不同的是，Sarsa采用在线策略（像水龙头洗手），从而更保守、期望更高；Q-learning采用离线策略（像水盆里洗手），从而更探索、期望更低

我认为并不能一概而论地说哪种算法更好，只能说在不同场景下，不同方法的侧重点不同

### Sarsa算法

使用表格Q存储当前策略下所有<状态，动作>对的价值。

#### 采用时序差分
>比较远离悬崖

#### 采用多步时序差分
>在里悬崖最远的路上走

### Q-learning
