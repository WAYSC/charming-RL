# 悬崖寻路

## 环境

`建立环境就是建立状态转移数组。`

>环境很重要，因为环境就是MDP。
>
>因为后面的操作需要你完全信任你自己写的状态转移数组（就像动态规划算法题里需要完全信任你的dp数组一样）

![悬崖寻路](https://github.com/WAYSC/charming-RL/blob/main/images/%E6%82%AC%E5%B4%96%E5%AF%BB%E8%B7%AF.png)

要求一个智能体从起点出发，避开悬崖行走，最终到达目标位置。如上图，有一个 4×12 的网格世界，每一个网格表示一个状态。智能体的起点是左下角的状态，目标是右下角的状态，智能体在每一个状态都可以采取 4 种动作：上、下、左、右。如果智能体采取动作后触碰到边界墙壁则状态不发生改变，否则就会相应到达下一个状态。环境中有一段悬崖，智能体掉入悬崖或到达目标状态都会结束动作并回到起点，也就是说掉入悬崖或者达到目标状态是终止状态。智能体每走一步的奖励是 −1，掉入悬崖的奖励是 −100。

------

状态转移数组P：[[[上][下][左][右]], [上][下][左][右]...col*row个...]

在每个格子都有上下左右四种可能的动作，每个动作包含四个元素，分别是(概率p, next_state, 即将获得的reward, 是否到出口/玩完done)

//个人觉得p在这里没有卵用，因为都赋为1

建立状态转移数组的逻辑就是：

1. 如果当前在悬崖，next_state=当前位置（已经掉下去了，动不了啦）, reward=0，玩完=True
2. 否则，计算下一步的位置，reward=-1，done=False（还未可知）👉 如果下一步悬崖，done=True玩完了；👉👉如果下一步悬崖且不是终点，reward=-100

好了，[环境](https://github.com/WAYSC/charming-RL/blob/main/Code/Hands-on%20Reinforcement%20Learning/cliff-walking/env.py)建好了，开始玩吧~

## 策略迭代
### 策略评估
先用下一状态的状态价值计算动作价值——
$$Q^π(s, a)=r(s, a)+r\sum_{s' \in s}P(s'|s, a)V^π(s')$$

```python
qsa = reward + self.gamma * self.v[next_state] * (1-done)
```

然后再用基于策略的所有动作价值计算这一状态的价值——
$$V^π(s)=\sum_{a\in A} π(a|s)Q^π(s, a)$$
```python
qsa_list.append(self.pi[s][a] * qsa) #对于同一位置的所有动作，s代表状态，a代表动作
new_v[s] = sum(qsa_list)
```
### 策略提升
```python
for s in range(self.env.row * self.env.col):
    qsa_list = []
    for a in range(4):
        p, next_state, reward, done = self.env.P[s][a]
        qsa = p * (reward + self.gamma * self.v[next_state] * (1-done))
        qsa_list.append(qsa) #此处不用乘π，乘了会出现死循环
    maxq = max(qsa_list)
    cntq = qsa_list.count(maxq) #数数有几个动作获得了最大价值
    self.pi[s] = [1 / cntq if q == maxq else 0 for q in qsa_list] #这些最大价值的动作均摊可能性
print("策略提升完成")
```

### 整体流程
反复循环策略评估->策略提升->策略评估->策略提升...直至策略π不再发生变化。

## 价值迭代
不存在显式的策略，只维护价值函数——
$$V^{k+1}(s)=max_{a\in A} [r(s, a)+γ\sum_{s' \in s}P(s'|s, a)V^k(s')]$$

当$v^{k+1}=V^k$时，也就是贝尔曼最优方程的不动点，用
$π(s)=argmax_a[r(s,a)+γ\sum_{s'}P(s'|s, a)V^{k+1}(s')]$来恢复最优策略π

# 冰湖
冰湖❄是gym里写好的环境，就不用我们自己配啦~

在冰湖环境中，价值迭代算法的结果和策略迭代算法的结果完全一致
