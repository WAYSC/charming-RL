# 悬崖寻路

## 环境

`建立环境就是建立状态转移数组。`

>环境很重要，因为环境就是MDP。
>
>因为后面的操作需要你完全信任你自己写的状态转移数组（就像动态规划算法题里需要完全信任你的dp数组一样）

![悬崖寻路](https://github.com/WAYSC/charming-RL/blob/main/images/%E6%82%AC%E5%B4%96%E5%AF%BB%E8%B7%AF.png)

要求一个智能体从起点出发，避开悬崖行走，最终到达目标位置。如上图，有一个 4×12 的网格世界，每一个网格表示一个状态。智能体的起点是左下角的状态，目标是右下角的状态，智能体在每一个状态都可以采取 4 种动作：上、下、左、右。如果智能体采取动作后触碰到边界墙壁则状态不发生改变，否则就会相应到达下一个状态。环境中有一段悬崖，智能体掉入悬崖或到达目标状态都会结束动作并回到起点，也就是说掉入悬崖或者达到目标状态是终止状态。智能体每走一步的奖励是 −1，掉入悬崖的奖励是 −100。

------

状态转移数组P：[[[上][下][左][右]], [上][下][左][右]...col*row个...]

在每个格子都有上下左右四种可能的动作，每个动作包含四个元素，分别是(概率p, next_state, 即将获得的reward, 是否到出口/玩完done)

//个人觉得p在这里没有卵用，因为都赋为1

建立状态转移数组的逻辑就是：

1. 如果当前在悬崖，next_state=当前位置（已经掉下去了，动不了啦）, reward=0，玩完=True
2. 否则，计算下一步的位置，reward=-1，done=False（还未可知）👉 如果下一步悬崖，done=True玩完了；👉👉如果下一步悬崖且不是终点，reward=-100

好了，[环境](https://github.com/WAYSC/charming-RL/blob/main/Code/Hands-on%20Reinforcement%20Learning/cliff-walking/env.py)建好了，开始玩吧~

## 策略迭代
### 策略评估
先用下一状态的状态价值计算动作价值——
$$Q^π(s, a)=r(s, a)+r\sum_{s' \in s}P(s'|s, a)V^π(s')$$

```
qsa = reward + self.gamma * self.v[next_state] * (1-done)
```

然后再用基于策略的所有动作价值计算这一状态的价值——
$$V^π(s)=\sum_{a\in A} π(a|s)Q^π(s, a)$$
```
qsa_list.append(self.pi[s][a] * qsa) #对于同一位置的所有动作，s代表状态，a代表动作
new_v[s] = sum(qsa_list)
```
### 策略提升
