## 第一章

>奖励的系数程度决定了游戏难度
>
>未来可以得到多少奖励取决于当前的状态和行为

### 基于策略迭代和基于价值迭代
直觉上，基于策略可能性较少（e.g.`Q-learning`）；基于价值又过于灵活（e.g.`策略梯度`）；如何trade off——根据策略做出动作、根据价值函数给出动作的价值（e.g.`Actor-Critic`）

### 探索-利用窘境
尝试次数一定，探索过多会损失很多最优解的机会；利用过多则可能选不到最优解。想要Reward最大，必须折衷。

### coding
上来安装gym就遇到了问题，报错是：
>Looking in indexes: （之前的镜像源）ERROR: Exception:Traceback (most recent call last)：[solution](https://zhuanlan.zhihu.com/p/395477058)

嚯！原来是憨批我开着梯子在用国内的镜像！

然后虚拟环境问题，[solution](https://blog.csdn.net/weixin_43907191/article/details/115707263)

最后还有一个问题，Pycharm导包不显示怎么办，[solution](https://blog.csdn.net/weixin_43526279/article/details/122922263?spm=1001.2101.3001.6650.2&utm_medium=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-2-122922263-blog-111855569.pc_relevant_multi_platform_whitelistv2&depth_1-utm_source=distribute.pc_relevant.none-task-blog-2%7Edefault%7ECTRLIST%7Edefault-2-122922263-blog-111855569.pc_relevant_multi_platform_whitelistv2&utm_relevant_index=5)

当第一个程序跑起来的时候，我真的是喜极而涕，RL的Hello World！诞生了！

![](https://github.com/WAYSC/charming-RL/blob/main/images/RL%20Hello%20World.gif)
